{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "seed = 2020"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchtext import data\n",
    "from torchtext.datasets import Multi30k\n",
    "from torchtext.data import Field, BucketIterator\n",
    "from torchtext.vocab import Vectors\n",
    "import time\n",
    "import pandas as pd\n",
    "import math\n",
    "import random\n",
    "from tqdm import tqdm\n",
    "from torchtext.vocab import GloVe\n",
    "from gensim.models.word2vec import Word2Vec\n",
    "from gensim.corpora.dictionary import Dictionary\n",
    "from nltk import word_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "torch.cuda.set_device(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenize = word_tokenize\n",
    "Article = data.Field(sequential=True, tokenize=tokenize, lower=True, init_token='<bos>', eos_token='<eos>',\n",
    "                  pad_token='<pad>', unk_token='<oov>')\n",
    "Title = data.Field(sequential=True, tokenize=tokenize, lower=True, eos_token='<eos>', \n",
    "                  pad_token='<pad>', unk_token='<oov>')\n",
    "Len = data.Field(sequential=False, use_vocab=False)\n",
    "ID = data.Field(sequential=False, use_vocab=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pmi_dct = dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_path = 'train_data.csv'\n",
    "valid_path = 'valid_data.csv'\n",
    "test_path = 'test_data.csv'\n",
    "\n",
    "\n",
    "class MyDataset(data.Dataset):\n",
    "    def __init__(self, csv_path, article_field, title_field, cat, **kwargs):\n",
    "\n",
    "        csv_data = pd.read_csv(csv_path)\n",
    "        fields = [(\"id\", ID), (\"article\", article_field), (\"title\", title_field), (\"art_len\", Len), (\"title_len\", Len)]\n",
    "        examples = []\n",
    "    \n",
    "        \n",
    "        for id, text, label in tqdm(zip(csv_data.index, csv_data['article'], csv_data['title'])):\n",
    "            examples.append(data.Example.fromlist([self.getID(id, cat), text, label, len(word_tokenize(text))+2, len(word_tokenize(label))+1], fields))\n",
    "        super(MyDataset, self).__init__(examples, fields)\n",
    "\n",
    "    def getID(self, id, cat):\n",
    "        if cat == 'train':\n",
    "            return id\n",
    "        elif cat == 'valid':\n",
    "            return id + 100000\n",
    "        elif cat == 'test':\n",
    "            return id + 110000\n",
    "        \n",
    "    def shuffle(self, text):\n",
    "        text = np.random.permutation(text.strip().split())\n",
    "        return ' '.join(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = MyDataset(train_path, article_field=Article, title_field=Title, cat='train')\n",
    "valid = MyDataset(valid_path, article_field=Article, title_field=Title, cat='valid')\n",
    "test = MyDataset(test_path, article_field=Article, title_field=Title, cat='test')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "vectors = Vectors(name='glove-wiki-gigaword-300.txt')\n",
    "Article.build_vocab(train, valid, test, vectors=vectors)\n",
    "default = Article.vocab.stoi['<oov>']\n",
    "Article.vocab.stoi.default_factory = lambda: default\n",
    "Title.vocab = Article.vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchtext.data import Iterator, BucketIterator\n",
    "train_iter, val_iter = BucketIterator.splits(\n",
    "        (train, valid), \n",
    "        batch_size=16, \n",
    "        device=device, \n",
    "        sort_key=lambda x: len(x.article),\n",
    "        sort_within_batch=True,\n",
    "        repeat=False \n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "\n",
    "import torch\n",
    "\n",
    "from torch.nn.parameter import Parameter\n",
    "from torch.nn.modules.module import Module\n",
    "\n",
    "\n",
    "class GraphConvolution(Module):\n",
    "    \"\"\"\n",
    "    Simple GCN layer, similar to https://arxiv.org/abs/1609.02907\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, in_features, out_features, device, bias=True):\n",
    "        super(GraphConvolution, self).__init__()\n",
    "        self.in_features = in_features\n",
    "        self.out_features = out_features\n",
    "        self.weight = Parameter(torch.FloatTensor(in_features, out_features)).to(device)\n",
    "        self.device = device\n",
    "        if bias:\n",
    "            self.bias = Parameter(torch.FloatTensor(out_features)).to(device)\n",
    "        else:\n",
    "            self.register_parameter('bias', None)\n",
    "        self.reset_parameters()\n",
    "\n",
    "    def reset_parameters(self):\n",
    "        stdv = 1. / math.sqrt(self.weight.size(1))\n",
    "        self.weight.data.uniform_(-stdv, stdv)\n",
    "        if self.bias is not None:\n",
    "            self.bias.data.uniform_(-stdv, stdv)\n",
    "\n",
    "    def forward(self, input, adj):\n",
    "        support = torch.mm(input, self.weight)\n",
    "        output = torch.mm(adj, support)\n",
    "        if self.bias is not None:\n",
    "            return output + self.bias\n",
    "        else:\n",
    "            return output\n",
    "\n",
    "    def __repr__(self):\n",
    "        return self.__class__.__name__ + ' (' \\\n",
    "               + str(self.in_features) + ' -> ' \\\n",
    "               + str(self.out_features) + ')'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch\n",
    "import math\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "class GCN(nn.Module):\n",
    "    def __init__(self, input_dim, nfeat, nhid, nout, dropout, device):\n",
    "        super(GCN, self).__init__()\n",
    "        self.input_dim = input_dim\n",
    "        self.nfeat = nfeat\n",
    "        self.gc1 = GraphConvolution(nfeat, nhid, device)\n",
    "        self.gc2 = GraphConvolution(nhid, nout, device)\n",
    "        self.device = device\n",
    "        self.dropout = dropout\n",
    "\n",
    "    def forward(self, input_seq, seq_embed, input_length, id=None):\n",
    "        pad_len = len(input_seq)\n",
    "        input_seq = input_seq[:input_length]\n",
    "        adj, uni_words = self.load_data(input_seq, id)\n",
    "        uni2idx = [list(input_seq).index(ele) for ele in uni_words]\n",
    "        seq2idx = [list(uni_words).index(ele) for ele in input_seq]\n",
    "        x = seq_embed[uni2idx]\n",
    "        x = F.relu(self.gc1(x, adj))\n",
    "        x = F.dropout(x, self.dropout, training=self.training)\n",
    "        x = self.gc2(x, adj)\n",
    "        x = torch.tanh(x) # [node_num, embded_dim]\n",
    "        x = x[seq2idx]  # [seq_len, embed_dim]\n",
    "        if pad_len > input_length:\n",
    "            diff = pad_len - input_length\n",
    "            pad_embed = torch.tensor(seq_embed[-1,:].unsqueeze(0).expand(diff, self.nfeat), device=self.device)\n",
    "            x = torch.cat((x,pad_embed),0)   # [pad_len, emb_dim]\n",
    "        return x\n",
    "        \n",
    "    def unranked_unique(self, nparray):\n",
    "        n_unique = len(np.unique(nparray))\n",
    "        ranked_unique = np.zeros([n_unique])\n",
    "        i = 0\n",
    "        for x in nparray:\n",
    "            if x not in ranked_unique:\n",
    "                ranked_unique[i] = x\n",
    "                i += 1\n",
    "        return ranked_unique\n",
    "\n",
    "    def load_data(self, word_seq, id):\n",
    "        if id is not None:\n",
    "            id = int(id)\n",
    "        word_seq = torch.tensor(word_seq, device=self.device)\n",
    "        words = torch.unique(word_seq, sorted=False)\n",
    "        word_num = len(words)\n",
    "        if id is None:\n",
    "            adj = self.get_pmi(word_seq, words, 3, self.device)\n",
    "        elif id in pmi_dct.keys():\n",
    "            adj = pmi_dct[id]\n",
    "        else:\n",
    "            adj = self.get_pmi(word_seq, words, 3, self.device)\n",
    "            adj = torch.tensor(self.normalize(adj), dtype=torch.float32, device=self.device)\n",
    "            pmi_dct[id] = adj\n",
    "        words = torch.tensor(words, dtype=torch.int64, device=self.device)\n",
    "        return adj, words\n",
    "\n",
    "    def get_pmi(self, word_seq, uni_words, window_size, device):\n",
    "        word_len = len(word_seq)\n",
    "        word_num = len(uni_words)\n",
    "        seq2idx = {int(ele): list(uni_words).index(ele) for ele in uni_words}\n",
    "        window_num = word_len - window_size + 1\n",
    "        if window_size > word_len:\n",
    "            window_size = word_len\n",
    "        win_num_matrix = torch.zeros((word_num, word_num), device=device)\n",
    "\n",
    "        for i in range(window_num):\n",
    "            words = list(torch.unique(word_seq[i:i+window_size], sorted=False))\n",
    "            indicies = [seq2idx[int(ele)] for ele in words]\n",
    "            for ele in indicies:\n",
    "                win_num_matrix[ele,indicies] += 1\n",
    "\n",
    "        pos_matrix = win_num_matrix / window_num\n",
    "        diag = torch.mul(pos_matrix, torch.eye(word_num, device=device)).inverse()\n",
    "        pos_matrix = torch.mm(diag, pos_matrix)\n",
    "        pos_matrix = torch.mm(pos_matrix, diag)\n",
    "        pmi_matrix = torch.clamp(torch.log(pos_matrix), min=0)\n",
    "        diag = torch.diag(pmi_matrix)\n",
    "        diag = 1 - diag\n",
    "        pmi_matrix = pmi_matrix + torch.diag_embed(diag)\n",
    "        return pmi_matrix\n",
    "\n",
    "    def normalize(self, mx):\n",
    "        \"\"\"Symmetrically normalize adjacency matrix\"\"\"\n",
    "        rowsum = mx.sum(1)\n",
    "        r_inv = torch.pow(rowsum, -0.5).flatten()\n",
    "        r_inv = torch.clamp(r_inv, min=0)\n",
    "        r_mat_inv = torch.diag(r_inv)\n",
    "        mx = torch.mm(r_mat_inv, mx)\n",
    "        mx = torch.mm(mx, r_mat_inv)\n",
    "        return mx\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder(nn.Module):\n",
    "    def __init__(self, gcn, input_dim, emb_dim, hid_dim, n_layers, device, dropout=0.5, bidirectional=True):\n",
    "        super(Encoder, self).__init__()\n",
    "        self.hid_dim = hid_dim\n",
    "        self.n_layers = n_layers\n",
    "        self.GCN = gcn\n",
    "        self.embedding = nn.Embedding(input_dim, emb_dim)\n",
    "        weight_matrix = Article.vocab.vectors\n",
    "        self.embedding.weight.data.copy_(weight_matrix)\n",
    "        self.device = device\n",
    "        self.gru = nn.GRU(emb_dim*2, hid_dim, n_layers, dropout=dropout, bidirectional=bidirectional)\n",
    "        \n",
    "    def forward(self, input_seqs, input_lengths, ids, hidden):\n",
    "        # input_seqs = [seq_len, batch]\n",
    "        embedded = self.embedding(input_seqs)\n",
    "        # embedded = [pad_len, batch, embed_dim]\n",
    "        gcn_outputs = torch.zeros(embedded.shape, device=self.device)\n",
    "        for i in range(embedded.shape[1]):\n",
    "            gcn_input = embedded[:,i,:]\n",
    "            if ids is not None:\n",
    "                gcn_output = self.GCN(input_seqs[:,i], gcn_input, input_lengths[i], ids[i]) # pad_len, emb_dim\n",
    "            else:\n",
    "                gcn_output = self.GCN(input_seqs[:,i], gcn_input, input_lengths[i])\n",
    "            gcn_outputs[:,i,:] = gcn_output\n",
    "        inputs = torch.cat((embedded, gcn_outputs),-1)\n",
    "        packed = torch.nn.utils.rnn.pack_padded_sequence(inputs, input_lengths, enforce_sorted=False)\n",
    "        \n",
    "        outputs, hidden = self.gru(packed, hidden)        \n",
    "        outputs, output_lengths = torch.nn.utils.rnn.pad_packed_sequence(outputs)\n",
    "        # outputs = [seq_len, batch, hid_dim * n directions]\n",
    "        # output_lengths = [batch]\n",
    "        return outputs, hidden\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Decoder(nn.Module):\n",
    "    def __init__(self, output_dim, emb_dim, hid_dim, n_layers, dropout=0.5, bidirectional=True):\n",
    "        super(Decoder, self).__init__()\n",
    "        \n",
    "        self.output_dim = output_dim\n",
    "        self.hid_dim = hid_dim\n",
    "        self.n_layers = n_layers\n",
    "        \n",
    "        self.embedding = nn.Embedding(output_dim, emb_dim)\n",
    "        weight_matrix = Article.vocab.vectors\n",
    "        self.embedding.weight.data.copy_(weight_matrix)\n",
    "        self.gru = nn.GRU(emb_dim, hid_dim, n_layers, dropout=dropout, bidirectional=bidirectional)\n",
    "        \n",
    "        if bidirectional:\n",
    "            self.fc_out = nn.Linear(hid_dim*2, output_dim)\n",
    "        else:\n",
    "            self.fc_out = nn.Linear(hid_dim, output_dim)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.softmax = nn.LogSoftmax(dim=1)\n",
    "        \n",
    "    def forward(self, token_inputs, hidden):\n",
    "        # token_inputs = [batch]\n",
    "        batch_size = token_inputs.size(0)\n",
    "        embedded = self.dropout(self.embedding(token_inputs).view(1, batch_size, -1))\n",
    "        # embedded = [1, batch, emb_dim]\n",
    "        output, hidden = self.gru(embedded, hidden)\n",
    "        # output = [1, batch,  n_directions * hid_dim]\n",
    "        # hidden = [n_layers * n_directions, batch, hid_dim]\n",
    "        \n",
    "        output = self.fc_out(output.squeeze(0))\n",
    "        output = self.softmax(output)\n",
    "        # output = [batch, output_dim]\n",
    "        return output, hidden"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Seq2Seq(nn.Module):\n",
    "    def __init__(self, \n",
    "                 encoder, \n",
    "                 decoder, \n",
    "                 device, \n",
    "                 predict=False, \n",
    "                 basic_dict=None,\n",
    "                 max_len=100\n",
    "                 ):\n",
    "        super(Seq2Seq, self).__init__()\n",
    "        \n",
    "        self.device = device\n",
    "\n",
    "        self.encoder = encoder\n",
    "        self.decoder = decoder\n",
    "\n",
    "        self.predict = predict  \n",
    "        self.basic_dict = basic_dict  \n",
    "        self.max_len = max_len  \n",
    "\n",
    "        self.enc_n_layers = self.encoder.gru.num_layers\n",
    "        self.enc_n_directions = 2 if self.encoder.gru.bidirectional else 1\n",
    "        self.dec_n_directions = 2 if self.decoder.gru.bidirectional else 1\n",
    "\n",
    "        assert encoder.hid_dim == decoder.hid_dim, \\\n",
    "            \"Hidden dimensions of encoder and decoder must be equal!\"\n",
    "        assert encoder.n_layers == decoder.n_layers, \\\n",
    "            \"Encoder and decoder must have equal number of layers!\"\n",
    "        assert self.enc_n_directions >= self.dec_n_directions, \\\n",
    "            \"If decoder is bidirectional, encoder must be bidirectional either!\"\n",
    "        \n",
    "    def forward(self, input_batches, input_lengths, target_batches=None, target_lengths=None, ids=None, teacher_forcing_ratio=0.5):\n",
    "        # input_batches = target_batches = [seq_len, batch]\n",
    "        batch_size = input_batches.size(1)\n",
    "        BOS_token = self.basic_dict[\"<bos>\"]\n",
    "        EOS_token = self.basic_dict[\"<eos>\"]\n",
    "        PAD_token = self.basic_dict[\"<pad>\"]\n",
    "\n",
    "        encoder_hidden = torch.zeros(self.enc_n_layers*self.enc_n_directions, batch_size, self.encoder.hid_dim, device=self.device)\n",
    "        \n",
    "        # encoder_output = [seq_len, batch, hid_dim * n directions]\n",
    "        # encoder_hidden = [n_layers*n_directions, batch, hid_dim]\n",
    "        encoder_output, encoder_hidden = self.encoder(\n",
    "            input_batches, input_lengths, ids, encoder_hidden)\n",
    "\n",
    "        decoder_input = torch.tensor([BOS_token] * batch_size, dtype=torch.long, device=self.device)\n",
    "        if self.enc_n_directions == self.dec_n_directions:\n",
    "            decoder_hidden = encoder_hidden\n",
    "        else:\n",
    "            L = encoder_hidden.size(0)\n",
    "            decoder_hidden = encoder_hidden[range(0, L, 2)] + encoder_hidden[range(1, L, 2)]\n",
    "\n",
    "        if self.predict:\n",
    "            assert batch_size == 1, \"batch_size of predict phase must be 1!\"\n",
    "            output_tokens = []\n",
    "\n",
    "            while True:\n",
    "                decoder_output, decoder_hidden = self.decoder(\n",
    "                    decoder_input, decoder_hidden\n",
    "                )\n",
    "                # [1, 1]\n",
    "                topv, topi = decoder_output.topk(1)\n",
    "                decoder_input = topi.squeeze(1)  \n",
    "                output_token = topi.squeeze().detach().item()\n",
    "                if output_token == EOS_token or len(output_tokens) == self.max_len:\n",
    "                    break\n",
    "                output_tokens.append(output_token)\n",
    "            return output_tokens\n",
    "\n",
    "        else:\n",
    "            max_target_length = max(target_lengths)\n",
    "            all_decoder_outputs = torch.zeros((max_target_length, batch_size, self.decoder.output_dim), device=self.device)\n",
    "\n",
    "            for t in range(max_target_length):\n",
    "                use_teacher_forcing = True if random.random() < teacher_forcing_ratio else False\n",
    "                if use_teacher_forcing:\n",
    "                    # decoder_output = [batch, output_dim]\n",
    "                    # decoder_hidden = [n_layers*n_directions, batch, hid_dim]\n",
    "                    decoder_output, decoder_hidden = self.decoder(\n",
    "                        decoder_input, decoder_hidden\n",
    "                    )\n",
    "                    all_decoder_outputs[t] = decoder_output\n",
    "                    decoder_input = target_batches[t]  \n",
    "                else:\n",
    "                    decoder_output, decoder_hidden = self.decoder(\n",
    "                        decoder_input, decoder_hidden\n",
    "                    )\n",
    "                    # [batch, 1]\n",
    "                    topv, topi = decoder_output.topk(1)\n",
    "                    all_decoder_outputs[t] = decoder_output\n",
    "                    decoder_input = topi.squeeze(1)  \n",
    "     \n",
    "            loss_fn = nn.NLLLoss(ignore_index=PAD_token)\n",
    "            loss = loss_fn(\n",
    "                all_decoder_outputs.reshape(-1,self.decoder.output_dim ),  # [batch*seq_len, output_dim]\n",
    "                target_batches.reshape(-1)                                                 # [batch*seq_len]\n",
    "            )\n",
    "            return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def epoch_time(start_time, end_time):\n",
    "    elapsed_time = end_time - start_time\n",
    "    elapsed_mins = int(elapsed_time / 60)\n",
    "    elapsed_secs = int(elapsed_time - (elapsed_mins * 60))\n",
    "    return elapsed_mins, elapsed_secs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(\n",
    "    model,\n",
    "    data_loader, \n",
    "    optimizer, \n",
    "    clip=1, \n",
    "    teacher_forcing_ratio=0.5, \n",
    "    print_every=1 \n",
    "    ):\n",
    "    model.predict = False\n",
    "    model.train()\n",
    "\n",
    "    if print_every == 0:\n",
    "        print_every = 1\n",
    "\n",
    "    print_loss_total = 0  \n",
    "    start = time.time()\n",
    "    epoch_loss = 0\n",
    "    step = 0\n",
    "    for batch in tqdm(data_loader, position=0, leave=True):\n",
    "        step += 1\n",
    "        ids = batch.id\n",
    "        input_batchs = batch.article\n",
    "        target_batchs = batch.title\n",
    "        input_lens = list(batch.art_len)\n",
    "        target_lens = list(batch.title_len)\n",
    "        optimizer.zero_grad()\n",
    "        loss = model(input_batchs, input_lens, target_batchs, target_lens, ids, teacher_forcing_ratio)\n",
    "        print_loss_total += loss.item()\n",
    "        epoch_loss += loss.item()\n",
    "        loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), clip)\n",
    "\n",
    "        optimizer.step()\n",
    "\n",
    "        if step % 500 == 0:\n",
    "            print_loss_avg = print_loss_total / 500\n",
    "            print_loss_total = 0\n",
    "            print('\\tCurrent Loss: %.4f' % print_loss_avg)\n",
    "\n",
    "    return epoch_loss / len(data_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(\n",
    "    model,\n",
    "    data_loader, \n",
    "    print_every=None\n",
    "    ):\n",
    "    model.predict = False\n",
    "    model.eval()\n",
    "    if print_every == 0:\n",
    "        print_every = 1\n",
    "\n",
    "    print_loss_total = 0  \n",
    "    start = time.time()\n",
    "    epoch_loss = 0\n",
    "    with torch.no_grad():\n",
    "        for i, batch in enumerate(data_loader):\n",
    "            ids = batch.id\n",
    "            input_batchs = batch.article\n",
    "            target_batchs = batch.title\n",
    "            input_lens = list(batch.art_len)\n",
    "            target_lens = list(batch.title_len)\n",
    "        \n",
    "\n",
    "            loss = model(input_batchs, input_lens, target_batchs, target_lens, ids, teacher_forcing_ratio=0)\n",
    "            print_loss_total += loss.item()\n",
    "            epoch_loss += loss.item()\n",
    "\n",
    "            if print_every and (i+1) % print_every == 0:\n",
    "                print_loss_avg = print_loss_total / print_every\n",
    "                print_loss_total = 0\n",
    "                print('\\tCurrent Loss: %.4f' % print_loss_avg)\n",
    "\n",
    "    return epoch_loss / len(data_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def summary(\n",
    "    model,\n",
    "    sample, \n",
    "    idx2token=None\n",
    "    ):\n",
    "    model.predict = True\n",
    "    model.eval()\n",
    "\n",
    "    # shape = [seq_len, 1]\n",
    "    input_batch = sample['src']\n",
    "    # list\n",
    "    input_len = sample['src_len']\n",
    "\n",
    "    output_tokens = model(input_batch, input_len)\n",
    "    output_tokens = [idx2token[t] for t in output_tokens]\n",
    "\n",
    "    return \" \".join(output_tokens)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "INPUT_DIM = len(Article.vocab.stoi)\n",
    "OUTPUT_DIM = len(Article.vocab.stoi)\n",
    "ENC_EMB_DIM = 300\n",
    "DEC_EMB_DIM = 300\n",
    "HID_DIM = 512\n",
    "N_LAYERS = 2\n",
    "ENC_DROPOUT = 0.5\n",
    "DEC_DROPOUT = 0.5\n",
    "LEARNING_RATE = 1e-4\n",
    "N_EPOCHS = 200\n",
    "CLIP = 1\n",
    "\n",
    "bidirectional = True\n",
    "gcn = GCN(INPUT_DIM, ENC_EMB_DIM, HID_DIM,ENC_EMB_DIM,0.5, device).to(device)\n",
    "enc = Encoder(gcn, INPUT_DIM, ENC_EMB_DIM, HID_DIM, N_LAYERS, device, ENC_DROPOUT, bidirectional)\n",
    "dec = Decoder(OUTPUT_DIM, DEC_EMB_DIM, HID_DIM, N_LAYERS, DEC_DROPOUT, bidirectional)\n",
    "model = Seq2Seq(enc, dec, device, basic_dict=Article.vocab.stoi).to(device)\n",
    "optimizer = optim.Adam(model.parameters(), lr=LEARNING_RATE)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_valid_loss = float('inf')\n",
    "count = 0\n",
    "\n",
    "for epoch in range(N_EPOCHS):\n",
    "    \n",
    "    start_time = time.time()\n",
    "    train_loss = train(model, train_iter, optimizer, CLIP)\n",
    "    valid_loss = evaluate(model, val_iter)\n",
    "    end_time = time.time()\n",
    "    if count == 5:\n",
    "        break\n",
    "    if valid_loss < best_valid_loss:\n",
    "        best_valid_loss = valid_loss\n",
    "        torch.save(model.state_dict(), 'gcn+seq2seq(node).pt')\n",
    "    else:\n",
    "        count += 1\n",
    "\n",
    "    epoch_mins, epoch_secs = epoch_time(start_time, end_time)\n",
    "    print(f'Epoch: {epoch+1:02} | Time: {epoch_mins}m {epoch_secs}s')\n",
    "        \n",
    "    print(f'\\tTrain Loss: {train_loss:.3f} | Val. Loss: {valid_loss:.3f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from rouge import Rouge\n",
    "\n",
    "model.load_state_dict(torch.load('gcn+seq2seq(node).pt'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dct = Article.vocab.stoi\n",
    "r1 = 0\n",
    "r2 = 0\n",
    "rl = 0\n",
    "rouge = Rouge()\n",
    "for i in range(10000):\n",
    "    sample = test[i].article\n",
    "    label = ' '.join(test[i].title)\n",
    "    sample = [dct['<bos>']] + [dct[ele] for ele in sample] + [dct['<eos>']]\n",
    "    test_sample = {}\n",
    "    test_sample[\"src\"] = torch.tensor(sample, dtype=torch.long, device=device).reshape(-1, 1)\n",
    "    test_sample[\"src_len\"] = [len(sample)]\n",
    "    predict = ' '.join(summary(model, test_sample, Article.vocab.itos))\n",
    "    score = rouge.get_scores(label, predict)[0]\n",
    "    r1 += score['rouge-1']['r']\n",
    "    r2 += score['rouge-2']['r']\n",
    "    rl += score['rouge-l']['r']\n",
    "#     print('--------------------------')\n",
    "    print(score['rouge-1']['r'],score['rouge-2']['r'],score['rouge-l']['r'])\n",
    "    print(i)\n",
    "    \n",
    "print(r1/10000)\n",
    "print(r2/10000)\n",
    "print(rl/10000)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
